import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import time
from typing import List

# FLOPsè¨ˆç®—ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™: pip install thop)
try:
    from thop import profile
except ImportError:
    # thopãŒãªã„å ´åˆã®ãƒ€ãƒŸãƒ¼è¨­å®š
    profile = None
    print("Warning: 'thop' not found. FLOPs will be calculated using a dummy value.")

# --- 1. åŸºæœ¬æ¼”ç®—ã‚¯ãƒ©ã‚¹ã®å®šç¾© ---

class Conv(nn.Module):
    """é€šå¸¸ã®ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯ (ReLU -> Conv2d -> BatchNorm2d)"""
    def __init__(self, C_in, C_out, kernel_size=3, stride=1, affine=True):
        super(Conv, self).__init__()
        padding = (kernel_size - 1) // 2
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),
            nn.BatchNorm2d(C_out, affine=affine)
        )
    def forward(self, x):
        return self.op(x)

class SepConv(nn.Module):
    """åˆ†é›¢å¯èƒ½ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯ (ReLU -> Depthwise Conv -> Pointwise Conv -> BatchNorm2d)"""
    def __init__(self, C_in, C_out, kernel_size=3, stride=1, affine=True):
        super(SepConv, self).__init__()
        padding = (kernel_size - 1) // 2
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=affine),
        )
    def forward(self, x):
        return self.op(x)


# --- 2. æ¢ç´¢çµæœã«åŸºã¥ãæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ (MNISTç”¨ã«èª¿æ•´) ---

class FinalNASModel(nn.Module):
    
    def __init__(self, num_classes=10, num_input_channels=1):
        super(FinalNASModel, self).__init__()
        
        layers = 17
        C = 63 # åˆæœŸãƒãƒ£ãƒãƒ«æ•° (Model Width)
        ops_code = [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
        kernel_sizes = [3] * layers
        
        # 1. ã‚¹ãƒ†ãƒ å±¤
        stem_multiplier = 2
        C_curr = stem_multiplier * C
        self.stem = nn.Sequential(
            nn.Conv2d(num_input_channels, C_curr, 3, padding=1, bias=False),
            nn.BatchNorm2d(C_curr)
        )
        
        # 2. ãƒŸãƒƒã‚¯ã‚¹å±¤ã®æ§‹ç¯‰
        C_prev = C_curr
        C_curr_base = C
        
        self.mixlayers = nn.ModuleList()
        
        for i in range(layers):
            # NetworkMixã®ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ (17å±¤ã®å ´åˆã€å±¤ 5, 11ã§ãƒãƒ£ãƒãƒ«å€å¢—)
            if i in [layers // 3, 2 * layers // 3]: # i=5 ã¨ i=11
                C_curr_base *= 2
                reduction = True
            else:
                reduction = False
                
            stride = 2 if reduction else 1
            kernel_size = kernel_sizes[i]
            C_out = C_curr_base

            if ops_code[i] == 0:
                mixlayer = SepConv(C_prev, C_out, kernel_size=kernel_size, stride=stride, affine=True)
            else:
                mixlayer = Conv(C_prev, C_out, kernel_size=kernel_size, stride=stride, affine=True)
            
            self.mixlayers.append(mixlayer)
            C_prev = C_out
            
        # 3. æœ€çµ‚å±¤
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(C_prev, num_classes)

    def forward(self, x):
        x = self.stem(x)
        for mixlayer in self.mixlayers:
            x = mixlayer(x)
        out = self.global_pooling(x)
        logits = self.classifier(out.view(out.size(0), -1))
        return logits


# --- 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é–¢æ•° ---

def load_mnist_data(batch_size=64):
    """MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader


# --- 4. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•° ---

def train(model, device, train_loader, optimizer, criterion, epoch):
    """ä¸€ã‚¨ãƒãƒƒã‚¯åˆ†ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    model.train()
    running_loss = 0.0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')
    
    avg_loss = running_loss / len(train_loader)
    return avg_loss

# --- 5. ãƒ†ã‚¹ãƒˆ/è©•ä¾¡é–¢æ•°ã¨FLOPsè¨ˆæ¸¬ ---

def test(model, device, test_loader, criterion, input_size):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆã¨FLOPsã®è¨ˆæ¸¬ã‚’è¡Œã†"""
    model.eval()
    test_loss = 0
    correct = 0
    
    # 1é †ä¼æ’­ã‚ãŸã‚Šã®FLOPsè¨ˆæ¸¬
    model_flops = 0.0
    if profile is not None:
        try:
            # ãƒ€ãƒŸãƒ¼å…¥åŠ› (Batch=1)
            dummy_input = torch.randn(1, *input_size).to(device)
            total_ops, _ = profile(model, inputs=(dummy_input,), verbose=False)
            model_flops = total_ops # thopã®å‡ºåŠ›å€¤ (æµ®å‹•å°æ•°ç‚¹æ¼”ç®—å›æ•°)
        except Exception as e:
            print(f"Error during FLOPs calculation with thop: {e}")
            model_flops = 1e8 # ãƒ€ãƒŸãƒ¼å€¤
    else:
        model_flops = 1e8 # thopãŒãªã„å ´åˆã®ãƒ€ãƒŸãƒ¼å€¤ (100 MFLOPsã‚’ä»®å®š)
    

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    
    print(f'\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({accuracy:.2f}%)')
          
    return accuracy, model_flops


# --- 6. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---

if __name__ == "__main__":
    
    # 1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    epochs = 20 # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°
    batch_size = 128
    learning_rate = 0.01
    
    # [æ–°è¦] ã‚³ã‚¹ãƒˆè¨ˆæ¸¬ã®ãŸã‚ã®ç›®æ¨™ç²¾åº¦
    TARGET_ACCURACY = 99.0 # ç›®æ¨™åˆ†é¡ç²¾åº¦ (%)
    
    # 2. ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
    train_loader, test_loader = load_mnist_data(batch_size=batch_size)
    
    # 4. ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æº–å‚™
    num_input_channels = 1 # MNISTã¯1ãƒãƒ£ãƒãƒ«
    input_size = (num_input_channels, 28, 28) # MNISTã®å…¥åŠ›ã‚µã‚¤ã‚º (C, H, W)
    
    model = FinalNASModel(num_classes=10, num_input_channels=num_input_channels).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)

    # 5. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚³ã‚¹ãƒˆè¨ˆæ¸¬ã®å®Ÿè¡Œ
    
    total_train_steps = len(train_loader.dataset) # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç·æ•°
    
    # ã‚³ã‚¹ãƒˆæŒ‡æ¨™ã®åˆæœŸåŒ–
    accumulated_flops = 0.0
    model_flops_per_inference = 0.0 # ãƒ¢ãƒ‡ãƒ«ã®1é †ä¼æ’­ã‚ãŸã‚Šã®FLOPs (ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã«è¨­å®š)

    print("\n--- Model Training Start with Cost Tracking ---")
    start_time = time.time()
    
    # åˆå›ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§ãƒ¢ãƒ‡ãƒ«ã®FLOPs/ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—
    initial_accuracy, model_flops_per_inference = test(model, device, test_loader, criterion, input_size)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¡¨ç¤º (thopãŒã‚ã‚‹å ´åˆã®ã¿)
    if profile is not None:
        dummy_input = torch.randn(1, *input_size).to(device)
        _, total_params = profile(model, inputs=(dummy_input,), verbose=False)
        print(f"Model Parameters: {total_params / 1e6:.2f} M")

    if initial_accuracy >= TARGET_ACCURACY:
        print(f"Goal met on initialization! Accuracy: {initial_accuracy:.2f}%. Cost: 0 FLOPs, 0s.")
    else:
        accuracy = initial_accuracy # åˆæœŸç²¾åº¦ã‚’è¨­å®š
        for epoch in range(1, epochs + 1):
            
            # --- è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ— ---
            train(model, device, train_loader, optimizer, criterion, epoch)
            
            # è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—å¾Œã®FLOPsã‚’ç´¯ç©
            # è¨“ç·´ã®FLOPsã¯ãƒ†ã‚¹ãƒˆã®ç´„3å€ (é †ä¼æ’­1 + é€†ä¼æ’­2)
            flops_per_epoch = model_flops_per_inference * total_train_steps * 3 
            accumulated_flops += flops_per_epoch
            
            # --- ãƒ†ã‚¹ãƒˆ/è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ— ---
            accuracy, _ = test(model, device, test_loader, criterion, input_size)
            
            # ç›®æ¨™ç²¾åº¦é”æˆã®ãƒã‚§ãƒƒã‚¯
            if accuracy >= TARGET_ACCURACY:
                end_time = time.time()
                wall_clock_time = end_time - start_time
                
                print("\n=========================================================")
                print(f"ğŸ‰ GOAL ACHIEVED! Accuracy {accuracy:.2f}% >= {TARGET_ACCURACY}%")
                print(f"ğŸ¯ Accumulated FLOPs Cost: {accumulated_flops:.2e}")
                print(f"â±ï¸ Wall-Clock Time Cost: {wall_clock_time:.2f} seconds")
                print("=========================================================")
                break
        else:
            # å…¨ã‚¨ãƒãƒƒã‚¯ã‚’å›ã—ãã£ãŸãŒã€ç›®æ¨™ç²¾åº¦ã«é”ã—ãªã‹ã£ãŸå ´åˆ
            end_time = time.time()
            wall_clock_time = end_time - start_time
            print(f"\n--- Training Finished (Goal Not Met) ---")
            print(f"Final Accuracy: {accuracy:.2f}%.")
            print(f"Max Accumulated FLOPs Cost: {accumulated_flops:.2e}")
            print(f"Total Wall-Clock Time: {wall_clock_time:.2f} seconds")