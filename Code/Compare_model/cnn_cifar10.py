import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import time

# FLOPsË®àÁÆó„ÅÆ„Åü„ÇÅ„ÅÆ„É©„Ç§„Éñ„É©„É™ („Ç§„É≥„Çπ„Éà„Éº„É´„ÅåÂøÖË¶Å„Åß„Åô: pip install thop)
try:
    from thop import profile
except ImportError:
    profile = None
    print("Warning: 'thop' not found. FLOPs will be calculated using a dummy value.")

# --- 1. Âü∫Êú¨ÊºîÁÆó„ÇØ„É©„Çπ„ÅÆÂÆöÁæ© ---

class Conv(nn.Module):
    """ÈÄöÂ∏∏„ÅÆÁï≥„ÅøËæº„Åø„Éñ„É≠„ÉÉ„ÇØ"""
    def __init__(self, C_in, C_out, kernel_size=3, stride=1, affine=True):
        super(Conv, self).__init__()
        padding = (kernel_size - 1) // 2
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),
            nn.BatchNorm2d(C_out, affine=affine)
        )
    def forward(self, x):
        return self.op(x)

class SepConv(nn.Module):
    """ÂàÜÈõ¢ÂèØËÉΩÁï≥„ÅøËæº„Åø„Éñ„É≠„ÉÉ„ÇØ"""
    def __init__(self, C_in, C_out, kernel_size=3, stride=1, affine=True):
        super(SepConv, self).__init__()
        padding = (kernel_size - 1) // 2
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=affine),
        )
    def forward(self, x):
        return self.op(x)


# --- 2. Êé¢Á¥¢ÁµêÊûú„Å´Âü∫„Å•„ÅèÊúÄÁµÇ„É¢„Éá„É´„ÇØ„É©„Çπ (CIFAR-10ÂØæÂøú) ---

class FinalNASModel(nn.Module):
    
    def __init__(self, num_classes=10, num_input_channels=3):
        super(FinalNASModel, self).__init__()
        
        layers = 17
        C = 63 # ÂàùÊúü„ÉÅ„É£„Éç„É´Êï∞
        ops_code = [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
        kernel_sizes = [3] * layers
        
        # 1. „Çπ„ÉÜ„É†Â±§ (ÂÖ•Âäõ„ÉÅ„É£„Éç„É´Êï∞„Å´Âêà„Çè„Åõ„Å¶Â§âÊõ¥)
        stem_multiplier = 2
        C_curr = stem_multiplier * C
        self.stem = nn.Sequential(
            nn.Conv2d(num_input_channels, C_curr, 3, padding=1, bias=False),
            nn.BatchNorm2d(C_curr)
        )
        
        # 2. „Éü„ÉÉ„ÇØ„ÇπÂ±§„ÅÆÊßãÁØâ
        C_prev = C_curr
        C_curr_base = C
        
        self.mixlayers = nn.ModuleList()
        
        for i in range(layers):
            if i in [layers // 3, 2 * layers // 3]:
                C_curr_base *= 2
                reduction = True
            else:
                reduction = False
                
            stride = 2 if reduction else 1
            kernel_size = kernel_sizes[i]
            C_out = C_curr_base

            if ops_code[i] == 0:
                mixlayer = SepConv(C_prev, C_out, kernel_size=kernel_size, stride=stride, affine=True)
            else:
                mixlayer = Conv(C_prev, C_out, kernel_size=kernel_size, stride=stride, affine=True)
            
            self.mixlayers.append(mixlayer)
            C_prev = C_out
            
        # 3. ÊúÄÁµÇÂ±§
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(C_prev, num_classes)

    def forward(self, x):
        x = self.stem(x)
        for mixlayer in self.mixlayers:
            x = mixlayer(x)
        out = self.global_pooling(x)
        logits = self.classifier(out.view(out.size(0), -1))
        return logits


# --- 3. „Éá„Éº„Çø„É≠„Éº„Éá„Ç£„É≥„Ç∞Èñ¢Êï∞ (CIFAR-10 & Augmentation) ---

def load_cifar10_data(batch_size=96):
    """CIFAR-10„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„É≠„Éº„Éâ„Åô„Çã"""
    
    # CIFAR-10„ÅÆÂπ≥Âùá„Å®Ê®ôÊ∫ñÂÅèÂ∑Æ
    CIFAR_MEAN = (0.4914, 0.4822, 0.4465)
    CIFAR_STD = (0.2023, 0.1994, 0.2010)

    # Ë®ìÁ∑¥Áî®: „Éá„Éº„ÇøÊã°Âºµ (Augmentation) „ÇíÈÅ©Áî®„Åó„Å¶ÈÅéÂ≠¶Áøí„ÇíÈò≤„Åê
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),      # „É©„É≥„ÉÄ„É†„Å´Âàá„ÇäÊäú„Åç
        transforms.RandomHorizontalFlip(),         # „É©„É≥„ÉÄ„É†„Å´Â∑¶Âè≥ÂèçËª¢
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD)
    ])

    # „ÉÜ„Çπ„ÉàÁî®: Ê≠£Ë¶èÂåñ„ÅÆ„Åø
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD)
    ])

    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=train_transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)

    test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=test_transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    return train_loader, test_loader


# --- 4. „Éà„É¨„Éº„Éã„É≥„Ç∞Èñ¢Êï∞ ---

def train(model, device, train_loader, optimizer, criterion, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.4f}')
    
    acc = 100. * correct / total
    print(f'End of Epoch {epoch}, Train Accuracy: {acc:.2f}%')


# --- 5. „ÉÜ„Çπ„Éà/Ë©ï‰æ°Èñ¢Êï∞„Å®FLOPsË®àÊ∏¨ ---

def test(model, device, test_loader, criterion, input_size):
    model.eval()
    test_loss = 0
    correct = 0
    
    # 1È†Ü‰ºùÊí≠„ÅÇ„Åü„Çä„ÅÆFLOPsË®àÊ∏¨
    model_flops = 0.0
    if profile is not None:
        try:
            dummy_input = torch.randn(1, *input_size).to(device)
            total_ops, _ = profile(model, inputs=(dummy_input,), verbose=False)
            model_flops = total_ops
        except Exception:
            model_flops = 1e8
    else:
        model_flops = 1e8

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    
    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({accuracy:.2f}%)')
          
    return accuracy, model_flops


# --- 6. „É°„Ç§„É≥ÂÆüË°å„Éñ„É≠„ÉÉ„ÇØ ---

if __name__ == "__main__":
    
    # 1. „Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„ÇøË®≠ÂÆö
    epochs = 50 # CIFAR-10„ÅØÈõ£„Åó„ÅÑ„ÅÆ„ÅßÂ§ö„ÇÅ„Å´Ë®≠ÂÆö
    batch_size = 128 # GPU„É°„É¢„É™„Å´Âøú„Åò„Å¶Ë™øÊï¥
    learning_rate = 0.025 # SGD„ÅÆÂàùÊúüÂ≠¶ÁøíÁéá
    
    # ÁõÆÊ®ôÁ≤æÂ∫¶ (CIFAR-10„ÅÆÂ†¥Âêà„ÄÅ90%„ÅØÈ´ò„ÅÑÂ£Å„Åß„Åô„ÄÇ„Åæ„Åö„ÅØ85-90%„ÇíÁõÆÊåá„Åó„Åæ„Åô)
    TARGET_ACCURACY = 90.0
    
    # 2. „Éá„Éê„Ç§„ÇπË®≠ÂÆö
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # 3. „Éá„Éº„Çø„É≠„Éº„ÉÄ„Éº„ÅÆÊ∫ñÂÇô
    train_loader, test_loader = load_cifar10_data(batch_size=batch_size)
    
    # 4. „É¢„Éá„É´Ê∫ñÂÇô (ÂÖ•Âäõ3„ÉÅ„É£„Éç„É´)
    num_input_channels = 3
    input_size = (3, 32, 32) # CIFAR-10 Size
    
    model = FinalNASModel(num_classes=10, num_input_channels=num_input_channels).to(device)
    criterion = nn.CrossEntropyLoss()
    
    # ÊúÄÈÅ©Âåñ: SGD + Momentum
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=3e-4)
    
    # Â≠¶ÁøíÁéá„Çπ„Ç±„Ç∏„É•„Éº„É©: Cosine Annealing (Â≠¶ÁøíÁéá„ÇíÂæê„ÄÖ„Å´‰∏ã„Åí„Å¶ÂèéÊùü„Åï„Åõ„Çã)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # 5. „Éà„É¨„Éº„Éã„É≥„Ç∞„Å®„Ç≥„Çπ„ÉàË®àÊ∏¨
    total_train_steps = len(train_loader.dataset)
    accumulated_flops = 0.0
    model_flops_per_inference = 0.0

    print("\n--- CIFAR-10 Training Start ---")
    start_time = time.time()
    
    # ÂàùÊúü„ÉÅ„Çß„ÉÉ„ÇØ
    initial_accuracy, model_flops_per_inference = test(model, device, test_loader, criterion, input_size)
    
    if profile is not None:
        dummy_input = torch.randn(1, *input_size).to(device)
        _, total_params = profile(model, inputs=(dummy_input,), verbose=False)
        print(f"Model Parameters: {total_params / 1e6:.2f} M")

    for epoch in range(1, epochs + 1):
        
        # --- Ë®ìÁ∑¥ ---
        train(model, device, train_loader, optimizer, criterion, epoch)
        
        # „Çπ„Ç±„Ç∏„É•„Éº„É©„ÅÆÊõ¥Êñ∞
        scheduler.step()
        
        # „Ç≥„Çπ„ÉàÁ¥ØÁ©ç (Train = Forward + Backward approx 3x FLOPs)
        flops_per_epoch = model_flops_per_inference * total_train_steps * 3 
        accumulated_flops += flops_per_epoch
        
        # --- „ÉÜ„Çπ„Éà ---
        accuracy, _ = test(model, device, test_loader, criterion, input_size)
        
        print(f"Current LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ÁõÆÊ®ôÈÅîÊàê„ÉÅ„Çß„ÉÉ„ÇØ
        if accuracy >= TARGET_ACCURACY:
            end_time = time.time()
            wall_clock_time = end_time - start_time
            
            print("\n=========================================================")
            print(f"üéâ GOAL ACHIEVED! Accuracy {accuracy:.2f}% >= {TARGET_ACCURACY}%")
            print(f"üéØ Accumulated FLOPs Cost: {accumulated_flops:.2e}")
            print(f"‚è±Ô∏è Wall-Clock Time Cost: {wall_clock_time:.2f} seconds")
            print("=========================================================")
            break
    else:
        end_time = time.time()
        wall_clock_time = end_time - start_time
        print(f"\n--- Training Finished (Goal Not Met) ---")
        print(f"Final Accuracy: {accuracy:.2f}%.")
        print(f"Total Wall-Clock Time: {wall_clock_time:.2f} seconds")